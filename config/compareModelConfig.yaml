# INITIAL SETTING
settings:
  data_directory: ../data/
  cv_test: 4
  cv_shuffle: 1 # 1 for True, 0 for false. O is important for potential temporal data
  model_name: best_model.pickle

# GLOBAL GRID SEARCH SETTINGS
grid_search:
  cv : 2

# GLOBAL MODELS 
input_models:
  reg :  [
          #"Linear Regression",
          "Ridge",
          "Lasso",
          "Elastic Net",
          "Decision Tree",
          #"Random Forest",
          #"Gradient Boosting",
          #"Extreme Gradient Boosting",
          #"Light GBM"
          ]
  classif : ["Logistic Regression",
          "Ridge",
          #"Lasso",
          #"Elastic Net",
          #"Decision Tree",
          #"Random Forest",
          #"KNN",
          #"Gradient Boosting",
          #"Extreme Gradient Boosting",
          #"Light GBM"
          ]

# GLOBAL PREPROCESSING PIPELINE PARAMETERS
pipeline_preprocessing:
  preprocesseurs_all_algo : ["simple", "poly","poly and spline"]
  poly_deg : 2 #3
  spline_knots : 5 #5
  spline_degree : 2 #3
  non_interaction_features : []
  scaler : ["Standard"] #"MinMax", "Standard", "Normalizer",

# GLOBAL EVALUATION
evaluation:
  reg:
    evaluation_metrics: ["RMSE","MAE",'MAPE',"Explained Variance", "R2 (not ajusted)","Max Residual Error","Min Residual Error","Q1 Residual Error","Q2 Residual Error","Q3 Residual Error","Max Residual Error2"]
  classif:
    evaluation_metrics: ['AUC','BIC']

# PARAMETRE DE REGRESSION
params_reg:
  params_simple:
    preproc: ["simple"] #,"poly","poly and spline"] #["simple"]
  params_ridge:
    preproc: ["poly","poly and spline"] #["simple"]
    alpha_start: -1 #p.logspace(-3, 3, 7)  # C values from 0.001 to 1000 (7 values in total)
    alpha_stop: 5
    alpha_num: 5
    max_iter: [3000]
  params_lasso:
    preproc: ["poly and spline"] #["simple"]
    alpha_start: -1
    alpha_stop: 5
    alpha_num: 5
    max_iter: [3000] # not usually a primary focus unless you encounter specific issues with convergence. In practice, 1000 is usually adequate, but if you get convergence warnings or have a very large/complex dataset, increasing it to 2000-5000 should handle most cases
    tol: [0.0001]  # default 0.0001
  params_elastic:
    preproc: ["poly","poly and spline"] #["simple"]
    alpha_start: -1
    alpha_stop: 5
    alpha_num: 3
    l1_start: 0.3 # entre 0 et 1
    l1_stop: 1
    l1_steps: 0.25
    max_iter: [3000]
    tol: [0.0001]  # default 0.0001
  params_tree:
    preproc: ["simple"] 
    min_samples_split : [2, 5]
  params_forest:
    preproc: ["simple"] 
    n_estimators : [50,100,200]  # Number of trees in random forest #[50,100,500,1000]
    max_depth : [10, 20, 30]  # Maximum number of levels in tree
    min_samples_split: [2, 5, 10]   # Minimum number of samples required to split a node
    min_samples_leaf : [1, 2, 4] # Minimum number of samples required at each leaf node
  params_gb:
    preproc: ["simple"] 
    n_estimators: [10, 100, 500]
    max_depth: [3, 5]
    learning_rate: [0.01, 0.1]
  params_xgb:
    preproc: ["simple"] 
    n_estimators: [100, 500, 1000] # the number of boosting rounds or trees to build.
    max_depth: [3, 5, 7] # maximum depth of a tree. Deeper trees can capture more complex patterns in the data, but may also lead to overfitting
    learning_rate: [0.01, 0.1]  # eta for learning rate A lower learning rate makes the model more cautious and slower to adjust
    min_child_weight:  [1, 3, 5] # The minimum sum of instance weights (also known as Hessian) needed in a child node
    subsample: [0.6, 1.0]  #  [0.6, 0.8, 1.0]The fraction of the training data used for training each tree.  Lowering this value can prevent overfitting by training on a smaller subset of the data
    colsample_bytree: [0.6, 1.0]  # [0.6, 0.8, 1.0] The fraction of features (columns) used when building each tree.
  params_lgbm:
    preproc: ["simple"] 
    n_estimators: [10, 100]
    max_depth: [90, 100]
    max_features: [2, 3]
    min_samples_leaf:  [3, 4]
    min_samples_split: [8, 10]
  
# PARAMETRES CLASSIFICATION
params_classif:
  param_global:
    threshold: 0.5
  params_simple:
    preproc: ["simple"] #,"poly","poly and spline"] #["simple"]
  params_ridge:
    preproc: ["simple"] #,["poly","poly and spline"] #["simple"]
    C_start: -2
    C_stop: 2
    C_num: 5
    max_iter: [3000]
  params_lasso:
    preproc: ["simple"] #["poly and spline"] #["simple"]
    C_start: -2
    C_stop: 2
    C_num: 5
    max_iter: [3000] # not usually a primary focus unless you encounter specific issues with convergence. In practice, 1000 is usually adequate, but if you get convergence warnings or have a very large/complex dataset, increasing it to 2000-5000 should handle most cases
    tol: [0.0001]  # default 0.0001
  params_elastic:
    preproc: ["poly"] #,"poly and spline"] #["simple"]
    C_start: -2
    C_stop: 2
    C_num: 3
    l1_start: 0.3 # entre 0 et 1
    l1_stop: 1
    l1_steps: 0.25
    max_iter: [3000]
    tol: [0.0001]  # default 0.0001
  params_knn:
    preproc: ["simple"] 
    n_neighbors : [5,10,20]
    p : [1, 2]
    weights : ['uniform', 'distance']
  params_tree:
    preproc: ["simple"] 
    min_samples_split : [2, 5]
  params_forest:
    preproc: ["simple"] 
    n_estimators : [50,100,200]  # Number of trees in random forest #[50,100,500,1000]
    max_depth : [10, 20, 30]  # Maximum number of levels in tree
    min_samples_split: [2, 5, 10]   # Minimum number of samples required to split a node
    min_samples_leaf : [1, 2, 4] # Minimum number of samples required at each leaf node
  params_gb:
    preproc: ["simple"] 
    n_estimators: [10, 100, 500]
    max_depth: [3, 5]
    learning_rate: [0.01, 0.1]
  params_xgb:
    preproc: ["simple"] 
    n_estimators: [100, 500, 1000] # the number of boosting rounds or trees to build.
    max_depth: [3, 5, 7] # maximum depth of a tree. Deeper trees can capture more complex patterns in the data, but may also lead to overfitting
    learning_rate: [0.01, 0.1]  # eta for learning rate A lower learning rate makes the model more cautious and slower to adjust
    min_child_weight:  [1, 3, 5] # The minimum sum of instance weights (also known as Hessian) needed in a child node
    subsample: [0.6, 1.0]  #  [0.6, 0.8, 1.0]The fraction of the training data used for training each tree.  Lowering this value can prevent overfitting by training on a smaller subset of the data
    colsample_bytree: [0.6, 1.0]  # [0.6, 0.8, 1.0] The fraction of features (columns) used when building each tree.
  params_lgbm:
    preproc: ["simple"] 
    n_estimators: [10, 100]
    max_depth: [90, 100]
    max_features: [2, 3]
    min_samples_leaf:  [3, 4]
    min_samples_split: [8, 10]
  