# INITIAL SETTING
settings:
  data_directory: ../data/
  cv_test: 10
  model_name: best_model.pickle

# GLOBAL GRID SEARCH SETTINGS
grid_search:
  cv : 5

# GLOBAL MODELS 
input_models:
  reg :  [
          #"Linear Regression",
          "Ridge",
          "Lasso",
          "Elastic Net",
          "Decision Tree",
          "Random Forest",
          "Gradient Boosting",
          "Extreme Gradient Boosting",
          #"Light GBM"
          ]
  classif : ["Logistic Regression",
          #"Ridge",
          #"Lasso",
          #"Elastic Net",
          #"Decision Tree",
          #"Random Forest",
          #"KNN",
          #"Gradient Boosting",
          #"Extreme Gradient Boosting",
          #"Light GBM"
          ]

# GLOBAL PREPROCESSING PIPELINE PARAMETERS
pipeline_preprocessing:
  preprocesseurs_all_algo : ["simple", "poly","poly and spline"]
  poly_deg : 2
  spline_knots : 2
  spline_degree : 2
  non_interaction_features : []
  scaler : ["Standard"] #"MinMax", "Standard", "Normalizer",

# GLOBAL EVALUATION
evaluation:
  reg:
    evaluation_metrics: ["RMSE","MAE",'MAPE',"Explained Variance", "R2 (not ajusted)","Max Residual Error","Min Residual Error","Q1 Residual Error","Q2 Residual Error","Q3 Residual Error","Max Residual Error2"]
  classif:
    evaluation_metrics: ['AUC','BIC']

# PARAMETRE DE REGRESSION
params_reg:
  params_simple:
    preproc: ["simple"] #,"poly","poly and spline"] #["simple"]
  params_ridge:
    preproc: ["poly","poly and spline"] #["simple"]
    alpha_start: -1
    alpha_stop: 5
    alpha_num: 5
    max_iter: [3000]
  params_lasso:
    preproc: ["poly and spline"] #["simple"]
    alpha_start: -1
    alpha_stop: 5
    alpha_num: 5
    max_iter: [3000] # not usually a primary focus unless you encounter specific issues with convergence. In practice, 1000 is usually adequate, but if you get convergence warnings or have a very large/complex dataset, increasing it to 2000-5000 should handle most cases
    tol: [0.0001]  # default 0.0001
  params_elastic:
    preproc: ["poly","poly and spline"] #["simple"]
    alpha_start: -1
    alpha_stop: 5
    alpha_num: 3
    l1_start: 0.3 # entre 0 et 1
    l1_stop: 1
    l1_steps: 0.25
    max_iter: [3000]
    tol: [0.0001]  # default 0.0001
  params_tree:
    preproc: ["simple"] 
    min_samples_split : [2, 5]
  params_forest:
    preproc: ["simple"] 
    n_estimators : [75,100,120]  # Number of trees in random forest #[50,100,500,1000]
    max_depth : [10, 50, 100]  # Maximum number of levels in tree
    min_samples_split: [2, 5, 10]   # Minimum number of samples required to split a node
    min_samples_leaf : [1, 2, 4] # Minimum number of samples required at each leaf node
  params_gb:
    preproc: ["simple"] 
    n_estimators: [10, 100, 500]
    max_depth: [3, 5]
    learning_rate: [0.01, 0.1]
  params_xgb:
    preproc: ["simple"] 
    n_estimators: [100, 500] # the number of boosting rounds or trees to build.
    max_depth: [3, 4, 5] # maximum depth of a tree. Deeper trees can capture more complex patterns in the data, but may also lead to overfitting
    learning_rate: [0.01, 0.1]  # eta for learning rate A lower learning rate makes the model more cautious and slower to adjust
    min_child_weight:  [1, 3, 5] # The minimum sum of instance weights (also known as Hessian) needed in a child node
    subsample: [0.6, 1.0]  #  [0.6, 0.8, 1.0]The fraction of the training data used for training each tree.  Lowering this value can prevent overfitting by training on a smaller subset of the data
    colsample_bytree: [0.6, 1.0]  # [0.6, 0.8, 1.0] The fraction of features (columns) used when building each tree.
  params_lgbm:
    preproc: ["simple"] 
    n_estimators: [10, 100]
    max_depth: [90, 100]
    max_features: [2, 3]
    min_samples_leaf:  [3, 4]
    min_samples_split: [8, 10]
  
# PARAMETRES CLASSIFICATION
params_classif:
  params_gb:
    learning_rate : [.01, .1]
    max_depth : [3, 7 ]
    subsample : [0.5, 1]
    n_estimators : [100, 200]
  params_rf:
    n_estimators : [10, 100]
    max_depth : [90, 100]
    max_features: [2, 3]
  params_knn:
    preprocessor__continuous__scaler : [StandardScaler(), MinMaxScaler(), Normalizer()]
    n_neighbors : [5,10]
    p : [1, 2]
    weights : ['uniform', 'distance']
  params_ridge:
  alpha_start: -1
  alpha_stop: 5
  alpha_num: 50
  max_iter: [1000]
params_lasso:
  alpha_start: -1
  alpha_stop: 5
  alpha_num: 50
  max_iter: [1000]
params_elastic:
  alpha_start: -1
  alpha_stop: 5
  alpha_num: 50
  max_iter: [1000]
params_gb:
  n_estimators: [10, 100]
  max_depth: [90, 100]
  max_features: [2, 3]
  #min_samples_leaf:  [3, 4]
 # min_samples_split: [8, 10]
params_xgb:
  n_estimators: [10, 100]
  max_depth: [90, 100]
  max_features: [2, 3]
  #min_samples_leaf:  [3, 4]
  #min_samples_split: [8, 10]
params_lgbm:
  n_estimators: [10, 100]
  max_depth: [90, 100]
  max_features: [2, 3]
  min_samples_leaf:  [3, 4]
  #min_samples_split: [8, 10]
  



